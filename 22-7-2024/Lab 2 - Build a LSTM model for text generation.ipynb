{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2 - Build a LSTM model for text generation\n",
    "#### Basic Steps for LSTM Text Generation\n",
    "##### 1. Load and Preprocess Data:\n",
    "- Load the text data from a source.\n",
    "- Convert the text to lowercase to ensure consistency.\n",
    "\n",
    "##### 2. Create Character Mappings:\n",
    "- Identify the unique characters in the text.\n",
    "- Create dictionaries to map characters to indices and vice versa.\n",
    "\n",
    "##### 3. Create Input Sequences:\n",
    "- Define the length of input sequences (maxlen).\n",
    "- Extract overlapping sequences of text (sentences) and their corresponding next characters (next_chars).\n",
    "\n",
    "##### 4. Vectorize the Data:\n",
    "- Convert the sequences and the next characters to one-hot encoded vectors (x for input sequences, y for next characters).\n",
    "\n",
    "##### 5. Build the LSTM Model:\n",
    "- Define a sequential model.\n",
    "- Add LSTM layers and a Dropout layer for regularization.\n",
    "- Add a Dense layer with a softmax activation function to predict the next character.\n",
    "\n",
    "##### 6. Compile the Model:\n",
    "- Compile the model using an optimizer (e.g., Adam) and a loss function (e.g., categorical cross-entropy).\n",
    "\n",
    "##### 7. Train the Model:\n",
    "- Train the model on the vectorized data using a specified batch size and number of epochs.\n",
    "- Optionally, use callbacks to monitor training progress and generate text at intervals.\n",
    "\n",
    "##### 8. Generate Text:\n",
    "- Define a function to sample the next character based on model predictions and a temperature parameter.\n",
    "- Generate text by predicting the next character iteratively, starting from a random seed sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import random\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, using CPU\n"
     ]
    }
   ],
   "source": [
    "# This cell of the code is useful if you have a GPU like Nvidia or If you are using a cloud platform like kaggle\n",
    "# Ensure GPU is being used\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "\u001b[1m600901/600901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3us/step\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the text data\n",
    "path = tf.keras.utils.get_file('nietzsche.txt', 'https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
    "text = text.lower()  # Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character-level mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = {char: i for i, char in enumerate(chars)}\n",
    "indices_char = {i: char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the text into sequences\n",
    "maxlen = 40  # Length of input sequences\n",
    "step = 1     # Step size to create sequences\n",
    "sentences = []  # Input sequences\n",
    "next_chars = []  # Output characters\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RNN model\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(maxlen, len(chars)), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128),\n",
    "    Dense(len(chars), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample the next character given the model's predictions\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text in each epoch\n",
    "def on_epoch_end(epoch, _):\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    for i in generated:\n",
    "        time.sleep(0.05)\n",
    "        print(i,end=\"\", flush=True)\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature=0.5)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        print(next_char,end=\"\", flush=True)\n",
    "    print()\n",
    "\n",
    "# Callback to generate text after each epoch\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4695/4695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 2.6227\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"many sentiments are lost to us is manife\"\n",
      "many sentiments are lost to us is manife condere the gorenthing and the more the pance the ham the pondiculle with the nend to ke ness to the end we the has his be and and to this the conderest of a condist the soming fect to with be the was distert of the everenty in the mento in the soment penconce of the the ress and to the ferem and is to the for to he suftes. which the for the pongence and their and ow be pocenting the and seal of \n",
      "\u001b[1m4695/4695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m973s\u001b[0m 206ms/step - loss: 2.6226\n",
      "Epoch 2/10\n",
      "\u001b[1m4695/4695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - loss: 1.9772\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"presses his way to the tragedy, the work\"\n",
      "presses his way to the tragedy, the work, the spectaring of the were\n",
      "which the more the porsent of the dore in the mort merse of a so for destentation of the it some who the streadition of all the our the pire of the some of a merience of the bection, in the presuction of is all the supress of a heral and the man and there conscuse of the mighes of the distince of the restarity of the moral of lan and has fore of destice is the geries o\n",
      "\u001b[1m4695/4695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1156s\u001b[0m 246ms/step - loss: 1.9772\n",
      "Epoch 3/10\n",
      "\u001b[1m4695/4695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - loss: 1.9103\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \" nothing that has caused me to meditate \"\n",
      " nothing that has caused me to meditate of the sencent suffert of other in the resions, has the porpare more of posserding mecespent, in the will seed and may will his still to a possions of the provession: and the procies and resied, and such and in the stime who is a propention of a feren and the love and presence of the\n",
      "sees and recire of the consest and prear all the semprection to the sention of the recenders that a some to jenting\n",
      "\u001b[1m4695/4695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1350s\u001b[0m 287ms/step - loss: 1.9103\n",
      "Epoch 4/10\n",
      "\u001b[1m4695/4695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - loss: 1.8263\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \" regard for the future and for one's hap\"\n",
      " regard for the future and for one's happose the sainst the and free understand the from the granst and the selfing the best not the streated the crate of the were and ender, a mind end which love the astem and the woman in the religion and the fermape and the now the cause, and more the say and his be fake of the somethand with the reality of the for our with all the man of the present the\n",
      "say, and interners of himself and realy itself\n",
      "\u001b[1m4695/4695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1376s\u001b[0m 293ms/step - loss: 1.8263\n",
      "Epoch 5/10\n",
      "\u001b[1m1197/4695\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:01\u001b[0m 275ms/step - loss: 1.7231"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T05:22:02.617463Z",
     "iopub.status.busy": "2024-05-21T05:22:02.617162Z",
     "iopub.status.idle": "2024-05-21T05:22:02.663147Z",
     "shell.execute_reply": "2024-05-21T05:22:02.662477Z",
     "shell.execute_reply.started": "2024-05-21T05:22:02.617437Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"LSTM for Text generation 90epocs 3 LSTM layers.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'conv_utils' from 'keras.utils' (c:\\Users\\bhawa\\miniconda3\\envs\\tester\\Lib\\site-packages\\keras\\api\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKERAS_BACKEND\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplaidml.keras.backend\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplaidml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mplaidml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstall_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Ensure GPU is being used\u001b[39;00m\n\u001b[0;32m     18\u001b[0m physical_devices \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bhawa\\miniconda3\\envs\\tester\\Lib\\site-packages\\plaidml\\keras\\__init__.py:68\u001b[0m, in \u001b[0;36minstall_backend\u001b[1;34m(import_path, backend, trace_file)\u001b[0m\n\u001b[0;32m     65\u001b[0m sys\u001b[38;5;241m.\u001b[39mmeta_path \u001b[38;5;241m=\u001b[39m [_PlaidMLBackendFinder(import_path, backend, trace_file)] \u001b[38;5;241m+\u001b[39m sys\u001b[38;5;241m.\u001b[39mmeta_path\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Hack around Keras expecting everything not Tensorflow to be Theano.\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m conv_utils\n\u001b[0;32m     69\u001b[0m conv_utils\u001b[38;5;241m.\u001b[39mconvert_kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'conv_utils' from 'keras.utils' (c:\\Users\\bhawa\\miniconda3\\envs\\tester\\Lib\\site-packages\\keras\\api\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Setup PlaidML\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()\n",
    "\n",
    "# Ensure GPU is being used\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")\n",
    "\n",
    "# Load and preprocess the text data\n",
    "path = tf.keras.utils.get_file('nietzsche.txt', 'https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
    "text = text.lower()  # Convert to lowercase\n",
    "\n",
    "# Create character-level mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = {char: i for i, char in enumerate(chars)}\n",
    "indices_char = {i: char for i, char in enumerate(chars)}\n",
    "\n",
    "# Cut the text into sequences\n",
    "maxlen = 40  # Length of input sequences\n",
    "step = 1     # Step size to create sequences\n",
    "sentences = []  # Input sequences\n",
    "next_chars = []  # Output characters\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "# Vectorization\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool_)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool_)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(maxlen, len(chars)), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128),\n",
    "    Dense(len(chars), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Function to sample the next character given the model's predictions\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# Function to generate text in each epoch\n",
    "def on_epoch_end(epoch, _):\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    for i in generated:\n",
    "        time.sleep(0.05)\n",
    "        print(i, end=\"\", flush=True)\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature=0.5)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        print(next_char, end=\"\", flush=True)\n",
    "    print()\n",
    "\n",
    "# Callback to generate text after each epoch\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "# Train the model\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
